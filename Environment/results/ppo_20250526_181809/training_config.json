{
    "algorithm": "ppo",
    "total_timesteps": 3000000,
    "reward_type": "balanced",
    "energy_weight": 0.1,
    "comfort_weight": 1.0,
    "model_path": "results/ppo_20250526_181809/logs/models/ppo_final_model",
    "training_date": "2025-05-26 19:13:06",
    "seed": 0,
    "device": "cpu",
    "n_envs": 4,
    "vec_env_type": "subproc",
    "normalized": true
}